{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df : dataframe.\n",
    "df_train = pd.read_csv(\"./Titanic_Project_Information_Data/train.csv\")\n",
    "df_test = pd.read_csv(\"./Titanic_Project_Information_Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./Titanic_Project_Information_Data/train.csv\", index_col=\"PassengerId\")\n",
    "df_train.head()\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_train_features = [\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
    "def convert_train_categorical(df, feature):\n",
    "    for feature in conv_train_features:\n",
    "        df[feature] = df[feature].astype(\"category\")\n",
    "\n",
    "convert_train_categorical(df_train, conv_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./Titanic_Project_Information_Data/test.csv\", index_col=\"PassengerId\")\n",
    "df_test.head()\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
    "def convert_test_categorical(df, feature):\n",
    "    for feature in conv_test_features:\n",
    "        df[feature] = df[feature].astype(\"category\")\n",
    "\n",
    "convert_test_categorical(df_test, conv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n",
    "df_train.describe(include=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis - EDA for Categorical : Survived, Sex, Embarked, Pclass (ordinal), SibSp, Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Survived\"].value_counts().to_frame()\n",
    "df_train[\"Survived\"].value_counts(normalize=True).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Sex\"].value_counts().to_frame()\n",
    "df_train[\"Sex\"].value_counts(normalize=True).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = [\"Sex\", \"Pclass\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
    "rows = 2\n",
    "cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(cols*3.5, rows*3.5))\n",
    "for r_row in range(rows):\n",
    "    for c_col in range(cols):\n",
    "        i = r_row * cols + c_col\n",
    "        if i < len(colums):\n",
    "            ax_i = axs[r_row, c_col]\n",
    "            sns.countplot(data=df_train, x=colums[i], hue=\"Survived\", ax=ax_i)\n",
    "            ax_i.set_title(f\"Survival rate by {colums[i]}\")\n",
    "            ax_i.legend(title=\"\", loc=\"upper right\", labels=[\"No\", \"Yes\"])\n",
    "\n",
    "axs.flat[-1].set_visible(False) # Hide the last subplot.\n",
    "plt.tight_layout()              # Adjust the layout.\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis - EDA for Numerical : (continuous) Age, Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x=\"Age\", bins=40, hue=\"Survived\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x=\"Fare\", bins=40, hue=\"Survived\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_fare = [\"Cheap\", \"Normal\", \"Expensive\", \"Luxury\"]\n",
    "pd.qcut(df_train[\"Fare\"], q=4, labels=categories_fare)\n",
    "\n",
    "sns.countplot(x=pd.qcut(df_train[\"Fare\"], q=4, labels=categories_fare), hue=\"Survived\", data=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering & Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - PassengerID(Name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_train[\"Name\"].tail(10)\n",
    "\n",
    "def title_extract(name):\n",
    "    title = re.compile(r\",([\\w\\s]+)\\.\") # Extract the title from the name.\n",
    "    return title.search(name).group(1).strip()\n",
    "\n",
    "df_train[\"Title\"]=df_train[\"Name\"].apply(lambda name: title_extract(name))\n",
    "df_train[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Title\"]=df_test[\"Name\"].apply(lambda name: title_extract(name))\n",
    "df_test[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group các title lại với nhau để giảm số lượng title.\n",
    "def title_group(title):\n",
    "    if title in [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]:\n",
    "        return title\n",
    "    elif title == \"Ms\":\n",
    "        return \"Miss\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "df_train[\"Title\"]=df_train[\"Title\"].apply(lambda title: title_group(title))\n",
    "df_test[\"Title\"]=df_test[\"Title\"].apply(lambda title: title_group(title))\n",
    "\n",
    "df_train[\"Title\"].value_counts()\n",
    "df_test[\"Title\"].value_counts()\n",
    "\n",
    "sns.countplot(data=df_train, x=\"Title\", hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Family(SibSp, Parch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"FamilySize\"] = df_train[\"SibSp\"].astype(int) + df_train[\"Parch\"].astype(int) + 1\n",
    "df_test[\"FamilySize\"] = df_test[\"SibSp\"].astype(int) + df_test[\"Parch\"].astype(int) + 1\n",
    "\n",
    "df_train[\"FamilyCate\"]=pd.cut(df_train[\"FamilySize\"], bins=[0, 1, 4, 6, 20], labels=[\"Single\", \"Small\", \"Medium\", \"Large\"])\n",
    "df_test[\"FamilyCate\"]=pd.cut(df_test[\"FamilySize\"], bins=[0, 1, 4, 6, 20], labels=[\"Single\", \"Small\", \"Medium\", \"Large\"])\n",
    "sns.countplot(data=df_train, x=\"FamilyCate\", hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Wrangling - Data Cleaning\n",
    "- Feature Numerical : Age, Fare\n",
    "- Feature Categorical : Sex, Pclass, Embarked, [Title(Name), FamilyCate(SibSp, Parch)] - Created in Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"Age\", \"Fare\"]\n",
    "categorical_features = [\"Sex\", \"Pclass\", \"Embarked\", \"Title\", \"FamilyCate\"]\n",
    "columns_features = numerical_features + categorical_features\n",
    "print(columns_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(df, columns_features):\n",
    "    for column in columns_features:\n",
    "        count_missing = df[column].isnull().sum()\n",
    "        if count_missing > 0:\n",
    "            print(f\"{column}: {count_missing} missing value(s) - {count_missing/len(df)*100:.2f}%\")\n",
    "          \n",
    "check_missing(df_train, columns_features)\n",
    "check_missing(df_test, columns_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tìm sự tương đồng giữa Age và cột dữ liệu khác, filling missing values  với median groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_by_sex_Pclass = df_train.groupby([\"Sex\", \"Pclass\"])[\"Age\"].median().to_frame()\n",
    "age_by_sex_Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Age\"]=df_train.groupby([\"Sex\", \"Pclass\"], observed=True)[\"Age\"].transform(lambda x: x.fillna(x.median())).to_frame()\n",
    "df_test[\"Age\"]=df_test.groupby([\"Sex\", \"Pclass\"], observed=True)[\"Age\"].transform(lambda x: x.fillna(x.median())).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(df_train, columns_features)\n",
    "check_missing(df_test, columns_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tách Dataframe_Train → X : Feature Columns & Y : Survived Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[columns_features]\n",
    "Y_train = df_train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[columns_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embarked, Fare - Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder: encode categorical features as a one-hot numeric array.\n",
    "# StandardScaler: standardize features by removing the mean and scaling to unit variance.\n",
    "# SimpleImputer: impute missing values.\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline: fill missing values, standardize features, and encode categorical features.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numer_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cate_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer: apply different transformers to different columns.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# define the preprocessor for numerical and categorical features.\n",
    "preprocessor = ColumnTransformer(transformers=[(\"numer\", numer_transformer, numerical_features),\n",
    "                                               (\"cate\", cate_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor to the training data with X_train.\n",
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data.\n",
    "X_train = preprocessor.transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation data.\n",
    "X_for_train, X_for_val, Y_for_train, Y_for_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_train.shape, X_for_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 : Binary Classification - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg=LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "logistic_reg.fit(X_for_train, Y_for_train)\n",
    "logistic_reg.score(X_for_train, Y_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the validation data and evaluate the model.\n",
    "y_pred = logistic_reg.predict(X_for_val)\n",
    "precision_score(Y_for_val, y_pred), recall_score(Y_for_val, y_pred), f1_score(Y_for_val, y_pred), accuracy_score(Y_for_val, y_pred)\n",
    "print(classification_report(Y_for_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 : Binary Classification - Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2) #degree=2: tạo ra các feature bậc 2.\n",
    "poly_features_Xtrain = poly.fit_transform(X_train)\n",
    "poly_features_Xval = poly.fit_transform(X_for_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "poly_log_reg.fit(poly_features_Xtrain, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg.score(poly_features_Xtrain, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the validation data and evaluate the model.\n",
    "y_pred = poly_log_reg.predict(poly_features_Xval)\n",
    "precision_score(Y_for_val, y_pred), recall_score(Y_for_val, y_pred), f1_score(Y_for_val, y_pred), accuracy_score(Y_for_val, y_pred)\n",
    "print(classification_report(Y_for_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 : Binary Classification - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, random_state=0)\n",
    "decision_tree.fit(X_for_train, Y_for_train)\n",
    "decision_tree.score(X_for_train, Y_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the validation data and evaluate the model.\n",
    "y_pred = decision_tree.predict(X_for_val)\n",
    "precision_score(Y_for_val, y_pred), recall_score(Y_for_val, y_pred), f1_score(Y_for_val, y_pred), accuracy_score(Y_for_val, y_pred)\n",
    "print(classification_report(Y_for_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 : Cross Validation k-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation: evaluate the model.\n",
    "logistic_reg_cross_val = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "decision_tree_cross_val = DecisionTreeClassifier(criterion=\"entropy\", max_depth=8, random_state=0)\n",
    "\n",
    "logistic_reg_scores = cross_val_score(logistic_reg_cross_val, X_train, Y_train, cv=5, scoring=\"accuracy\")\n",
    "logistic_reg_scores.mean(), logistic_reg_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_scores = cross_val_score(decision_tree_cross_val, X_train, Y_train, cv=5, scoring=\"accuracy\")\n",
    "decision_tree_scores.mean(), decision_tree_scores.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
